{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from Keywords import *\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "from rake_nltk import Rake, Metric\n",
    "import yake\n",
    "\n",
    "df = pd.read_csv('CleanedPolite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix(s: str, end: str, add: str = '', ignore_errors: bool = True):\n",
    "    \"\"\"\n",
    "    Returns a string (s) up to before the the cut off str (end) and appends \n",
    "    (add) to string. If end can't be found, this returns the original string, \n",
    "    but there is option to raise exception if end is not found.\n",
    "    \"\"\"\n",
    "    if ignore_errors:\n",
    "        i = s.rfind(end)\n",
    "    else:\n",
    "        i = s.rindex(end)\n",
    "    return s[:i] + add\n",
    "\n",
    "# Preps email row before keyword extraction\n",
    "def get_complete_body(data_row: Series) -> str:\n",
    "    \"\"\"\n",
    "    Given a row of email data, returns a complete body of data about the email.\n",
    "    This adjusts the subject and description of the email before returning\n",
    "    their combination.\n",
    "\n",
    "    Input:\n",
    "        data_row: row corresponding to a single direct inject\n",
    "\n",
    "    Returns: str of a cleaned email description, ready for keyword extraction\n",
    "    \"\"\"\n",
    "    s_email = shorten_email(data_row['description']).lower()\n",
    "    s_subject = remove_suffix(data_row['Subject'][3:], ' - ') #Removes Q: & name\n",
    "    complete_body = s_subject + ', ' + s_email\n",
    "    complete_body = complete_body.lower()\n",
    "\n",
    "    return complete_body\n",
    "\n",
    "# Trying out YAKE\n",
    "def yake_extract(complete_body: str) -> list | list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Given a complete_body string, returns a list of keyword phrases and their \n",
    "    scores from the yake_extractor. Lower score is better.\n",
    "    \"\"\"\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 5\n",
    "    deduplication_threshold = 0.9\n",
    "    windowSize = 3\n",
    "    numOfKeywords = 6\n",
    "\n",
    "    yake_kw_extractor = yake.KeywordExtractor(\n",
    "        lan = language, \n",
    "        n = max_ngram_size, \n",
    "        dedupLim = deduplication_threshold, \n",
    "        windowsSize = windowSize, \n",
    "        top = numOfKeywords\n",
    "    )\n",
    "\n",
    "    key_phrases = yake_kw_extractor.extract_keywords(complete_body)\n",
    "    return key_phrases\n",
    "\n",
    "# Trying out RAKE\n",
    "def rake_extract(complete_body: str) -> list[tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Given a complete_body string, returns a list of keyword phrases and their \n",
    "    scores from the yake_extractor. Higher score is better.\n",
    "    \"\"\"\n",
    "    stop = set(stopwords.words(\"english\")) # Not sure if they do anything\n",
    "    rake_extractor = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
    "    rake_extractor.extract_keywords_from_text(complete_body)\n",
    "    return rake_extractor.get_ranked_phrases_with_scores()\n",
    "\n",
    "# Combining Yake and Rake\n",
    "def get_comment_yake(y_phrases, r_phrases) -> str:\n",
    "    \"\"\"\n",
    "    For some algorithm of choosing keywords (TBD), this takes the yake and rake\n",
    "    keywords and returns a fitting email comment.\n",
    "\n",
    "    Inputs:\n",
    "        y_phrases: list of keyword phrases from Yake using complete_body\n",
    "        r_phrases: list of keyword phrases from Rake using complete_body\n",
    "\n",
    "    Returns: str of the email comment\n",
    "    \"\"\"\n",
    "    for r in r_phrases:\n",
    "        for y in y_phrases:\n",
    "            if r in y:\n",
    "                return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass test: create comment for every email row\n",
    "comments = []\n",
    "for i, row in df.iterrows():\n",
    "    a = get_complete_body(row)\n",
    "    b = yake_extract(a)\n",
    "    c = rake_extract(a)\n",
    "    \n",
    "    y = [i for i,j in b]\n",
    "    r = [j for i,j in c]\n",
    "\n",
    "    comments.append(get_comment_yake(y,r))\n",
    "\n",
    "mycomments = pd.Series(comments)\n",
    "mycomments.to_csv('RakeOutputs.csv') #Currently doesn't merge into df\n",
    "# df['Test Comments'] = comments\n",
    "# df.to_csv('TestOutput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "editing templates\n",
      "AAAAAA politemail help,  hi,  is there someone available to walk me through building and editing templates? i am having difficulty editing/changing the formatting for existing templates from my predecessor, specifically with certain images not displaying and other background issues.  ashley wynstra communications manager  campus and student life the university of chicago behar family house 5711 s. woodlawn ave. chicago, il 60637 awynstra@uchicago.edu<mailto:awynstra@uchicago.edu\n",
      "YYYYYYY ['walk me through building', 'building and editing templates', 'uchicago.edu', 'politemail', 'changing the formatting for existing', 'formatting for existing templates']\n",
      "RRRRRRR ['chicago behar family house 5711', 'ashley wynstra communications manager campus', 'il 60637 awynstra', 'editing templates', 'existing templates', 'difficulty editing', 'woodlawn ave', 'uchicago', 'uchicago', 'student life', 'someone available', 'politemail help', 'edu', 'edu', 'chicago', 'certain images', 'background issues', 'awynstra', 'walk', 'university', 'specifically', 'predecessor', 'mailto', 'hi', 'formatting', 'displaying', 'changing', 'building']\n",
      "Email To: politemailsupport@uchicago.edu  Email From: awynstra@uchicago.edu  Email Text: Hi,  Is there someone available to walk me through building and editing templates? I am having difficulty editing/changing the formatting for existing templates from my predecessor, specifically with certain images not displaying and other background issues.  Ashley Wynstra Communications Manager  Campus and Student Life The University of Chicago Behar Family House 5711 S. Woodlawn Ave. Chicago, IL 60637 awynstra@uchicago.edu<mailto:awynstra@uchicago.edu>\n"
     ]
    }
   ],
   "source": [
    "# Notes and Tests on individual rows\n",
    "mini_test_rows = [2] #Ones that worked previously\n",
    "test_row = df.iloc[2] #row 6 shows i can't assume email ends after thank you\n",
    "\n",
    "a = get_complete_body(test_row)\n",
    "b = yake_extract(a)\n",
    "c = rake_extract(a)\n",
    "\n",
    "y = [i for i,j in b]\n",
    "r = [j for i,j in c]\n",
    "\n",
    "print(get_comment_yake(y,r))\n",
    "print('AAAAAA', a)\n",
    "print('YYYYYYY', y)\n",
    "print('RRRRRRR', r)\n",
    "print(test_row['description'])\n",
    "\n",
    "# further analysis of pos tags on these yake rake keywords\n",
    "# sentences = nltk.sent_tokenize(a)\n",
    "# words = [word for sent in sentences for word in nltk.word_tokenize(sent)]\n",
    "# nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Analysis on pos tags\n",
    "# Counting number of pos tags and words\n",
    "df['Comment'].apply(lambda x: len(word_tokenize(x))).value_counts()\n",
    "df['Comment'][df['Comment'].apply(lambda x: len(word_tokenize(x))) >= 5]\n",
    "\n",
    "tag_count: dict[str, int] = {}\n",
    "tag_word: dict[str, list[str]] = {}\n",
    "comment_tags = df['Comment'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
    "for tag_list in comment_tags:\n",
    "    for tag in tag_list:\n",
    "        if tag_count.get(tag[1]) is None:\n",
    "            tag_count[tag[1]] = 1\n",
    "            tag_word[tag[1]] = [tag[0]]\n",
    "        else:\n",
    "            entry = tag_count[tag[1]]\n",
    "            tag_count[tag[1]] += 1\n",
    "            tag_word[tag[1]].append(tag[0])\n",
    "\n",
    "tag_count\n",
    "# print(len(set(tag_word['NNP'])))\n",
    "word_count = {word: tag_word['NNP'].count(word) for word in tag_word['NNP']}\n",
    "# word_count = np.array(list(word_count.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
