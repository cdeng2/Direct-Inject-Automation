{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from Keywords import *\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "from rake_nltk import Rake, Metric\n",
    "import yake\n",
    "\n",
    "df = pd.read_csv('CleanedPolite.csv')\n",
    "# series_nltk = df['description'].apply(get_comment_from_pos)\n",
    "# print(series_nltk)\n",
    "\n",
    "# test_row = df.iloc[2]\n",
    "\n",
    "def remove_suffix(s: str, end: str, add: str = '', ignore_errors: bool = True):\n",
    "    \"\"\"\n",
    "    Returns a string (s) up to before the the cut off str (end) and appends \n",
    "    (add) to string. If end can't be found, this returns the original string, \n",
    "    but there is option to raise exception if end is not found.\n",
    "    \"\"\"\n",
    "    if ignore_errors:\n",
    "        i = s.rfind(end)\n",
    "    else:\n",
    "        i = s.rindex(end)\n",
    "    return s[:i] + add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_body(data_row: Series) -> str:\n",
    "    \"\"\"\n",
    "    Given a series with the below columns, returns the subject line with\n",
    "    email body. Adjustments made to the final string.\n",
    "    \"\"\"\n",
    "    s_email = shorten_email(data_row['description']).lower()\n",
    "    s_subject = remove_suffix(data_row['Subject'][3:], ' - ')\n",
    "    complete_body = s_subject + ', ' + s_email\n",
    "    complete_body = complete_body.lower()\n",
    "\n",
    "    return complete_body\n",
    "\n",
    "# Trying out YAKE\n",
    "def yake_extract(complete_body: str) -> list | list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Given a complete_body string generated from get_complete_body(), returns\n",
    "    a list of keyword phrases and their score from the yake_extractor. Lower\n",
    "    score is better.\n",
    "    \"\"\"\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 5\n",
    "    deduplication_threshold = 0.9\n",
    "    windowSize = 3\n",
    "    numOfKeywords = 6\n",
    "\n",
    "    yake_kw_extractor = yake.KeywordExtractor(\n",
    "        lan = language, \n",
    "        n = max_ngram_size, \n",
    "        dedupLim = deduplication_threshold, \n",
    "        windowsSize = windowSize, \n",
    "        top = numOfKeywords\n",
    "    )\n",
    "\n",
    "    key_phrases = yake_kw_extractor.extract_keywords(complete_body)\n",
    "    return key_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out RAKE\n",
    "def rake_extract(complete_body: str) -> list[tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Given a complete_body string generated from get_complete_body(), returns\n",
    "    its keyword phrases with their scores. For Rake, a higher score is better.\n",
    "    \"\"\"\n",
    "    stop = set(stopwords.words(\"english\")) # Not sure if they do anything\n",
    "    rake_extractor = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
    "    rake_extractor.extract_keywords_from_text(complete_body)\n",
    "    return rake_extractor.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Yake and Rake\n",
    "def get_comment_yake(y_phrases, r_phrases) -> str:\n",
    "    \"\"\"\n",
    "    Returns a generated email comment. \n",
    "\n",
    "    Inputs:\n",
    "        y_phrases: list of keyword phrases from Yake using complete_body\n",
    "        r_phrases: list of keyword phrases from Rake using complete_body\n",
    "\n",
    "    Returns: str of the email comment\n",
    "    \"\"\"\n",
    "    for r in r_phrases:\n",
    "        for y in y_phrases:\n",
    "            if r in y:\n",
    "                return y\n",
    "\n",
    "comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    a = get_complete_body(row)\n",
    "    b = yake_extract(a)\n",
    "    c = rake_extract(a)\n",
    "    \n",
    "    y = [i for i,j in b]\n",
    "    r = [j for i,j in c]\n",
    "\n",
    "    comments.append(get_comment_yake(y,r))\n",
    "\n",
    "df['Test Comments'] = comments\n",
    "df.to_csv('TestOutput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes in a politemail template\n",
      "AAAAAA building buttons/shapes in a politemail template,  greetings,  \n",
      "YYYYYYY ['shapes in a politemail template', 'building buttons', 'shapes in a politemail', 'politemail template', 'building', 'buttons']\n",
      "RRRRRRR ['politemail template', 'building buttons', 'shapes', 'greetings']\n",
      "Email To: politemailsupport@uchicago.edu  Email From: bransom@uchicago.edu  Email Text: Greetings,  Thanks again for your PoliteMail training a few weeks ago. I'm starting to deploy messages and things are going well.  I'm wondering if you could help me navigate an issue with building shapes and buttons in a template that are well-centered in the message. I'd like to build something resembling a button (with a \"Watch Video\" CTA) that would be perfectly centered. I'm having trouble with a particular case where the button is slightly off center and things don't quite look right. I can forward to you to give you a sense of this.  Would you be willing to talk through this and help me? My Outlook calendar is up to date (bransom@uchicago.edu) so don't hesitate to go ahead and set up a meeting if there's a time that works.  Once again, grateful for your help on this.  All the best, Benjamin\n"
     ]
    }
   ],
   "source": [
    "mini_test_rows = [2] #Ones that worked previously\n",
    "test_row = df.iloc[6] #row 6 shows i can't assume email ends after thank you\n",
    "a = get_complete_body(test_row)\n",
    "b = yake_extract(a)\n",
    "c = rake_extract(a)\n",
    "\n",
    "y = [i for i,j in b]\n",
    "r = [j for i,j in c]\n",
    "\n",
    "print(get_comment_yake(y,r))\n",
    "print('AAAAAA', a)\n",
    "print('YYYYYYY', y)\n",
    "print('RRRRRRR', r)\n",
    "print(test_row['description'])\n",
    "# print(test_row['Comment'])\n",
    "\n",
    "# sentences = nltk.sent_tokenize(a)\n",
    "# words = [word for sent in sentences for word in nltk.word_tokenize(sent)]\n",
    "# nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of pos tags and words\n",
    "df['Comment'].apply(lambda x: len(word_tokenize(x))).value_counts()\n",
    "df['Comment'][df['Comment'].apply(lambda x: len(word_tokenize(x))) >= 5]\n",
    "\n",
    "tag_count: dict[str, int] = {}\n",
    "tag_word: dict[str, list[str]] = {}\n",
    "comment_tags = df['Comment'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
    "for tag_list in comment_tags:\n",
    "    for tag in tag_list:\n",
    "        if tag_count.get(tag[1]) is None:\n",
    "            tag_count[tag[1]] = 1\n",
    "            tag_word[tag[1]] = [tag[0]]\n",
    "        else:\n",
    "            entry = tag_count[tag[1]]\n",
    "            tag_count[tag[1]] += 1\n",
    "            tag_word[tag[1]].append(tag[0])\n",
    "\n",
    "tag_count\n",
    "# print(len(set(tag_word['NNP'])))\n",
    "word_count = {word: tag_word['NNP'].count(word) for word in tag_word['NNP']}\n",
    "# word_count = np.array(list(word_count.values()))\n",
    "# word_count\n",
    "# word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
